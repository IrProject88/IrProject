{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluate_query_expansion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s8LB7-0Pg9T",
        "colab_type": "code",
        "outputId": "3a4a8fbd-b413-4e14-ae59-97f9e5d23278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip install rank_bm25\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import heapq"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.6/dist-packages (0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank_bm25) (1.17.4)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvUFjyLWRwgm",
        "colab_type": "code",
        "outputId": "f8542cc0-9cbe-48e0-9f7e-31457c87e2bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABKTKno5SEgw",
        "colab_type": "text"
      },
      "source": [
        "#Load Document Judgements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxSIagF6QoSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath =  \"/content/drive/My Drive/processed/document_judgments.csv\"\n",
        "judgements = pd.read_csv(filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htJmyeFrSKEG",
        "colab_type": "text"
      },
      "source": [
        "#Load Queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j594dGzzR6WI",
        "colab_type": "code",
        "outputId": "528a676c-24b1-4746-b390-1fa5efbfe471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "queries_file =  \"/content/drive/My Drive/2018CommonCoreTrack/expanded_queries_final.csv\"\n",
        "queries_df = pd.read_csv(queries_file)\n",
        "print(queries_df.columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0', 'topic_id', 'title', 'in_out_q', 'in_in_q'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4Nq0ZEot90D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def prepros_query(query):\n",
        "    tokenized_query = [w.lower() for w in nltk.word_tokenize(query) if w not in stop_words and w != ',' and  w != \"' '\"]\n",
        "    return tokenized_query"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf_W3vLhYjsY",
        "colab_type": "text"
      },
      "source": [
        "# Discounted Cumulative Gain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oa_Q06LdyTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gain(rank, topic_id):\n",
        "    DC = 0\n",
        "    ID = 0 \n",
        "    for i in range(1, len(rank)+1):\n",
        "        judgement = (judgements.loc[(judgements.topic_id == topic_id) & (judgements.document_id == rank[i-1])][\"judgment\"])\n",
        "        if  not len(judgement) == 0:\n",
        "            rel = judgement.iloc[0]\n",
        "        else:\n",
        "            rel = 0 \n",
        "        if i > 1:\n",
        "            DC = DC + rel/ np.log2(i)\n",
        "        else:\n",
        "            DC = rel\n",
        "    for i in range(1, len(rank)+1):\n",
        "        rel = 0\n",
        "        if i < len(ideal):\n",
        "            rel =  ideal.iloc[i-1][3]\n",
        "        if i > 1:\n",
        "            ID = ID + rel/ np.log2(i)\n",
        "        else:\n",
        "            ID = rel\n",
        "    nDC = DC/ID\n",
        "    return nDC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTi3vsxtR9MC",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate on non-Stemmed Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX0b2VW1U8_9",
        "colab_type": "text"
      },
      "source": [
        "## Load Tokenized Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFa3F_RWl_gK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pathfile = \"/content/drive/My Drive/processed/tokenized_data.parquet\"\n",
        "tokenized_df = pd.read_parquet(pathfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqP3OHsGS1bV",
        "colab_type": "text"
      },
      "source": [
        "Convert to list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeRjxWv1ROui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_c = []\n",
        "for x in range(len(tokenized_df)):\n",
        "    tokenized_c.append(tokenized_df.iloc[x][2].tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1opnC8ssWAA5",
        "colab_type": "code",
        "outputId": "7177d0eb-393a-4c71-b829-b81968e98ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tokenized_c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "592341"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wns9Cy0iSS7Z",
        "colab_type": "text"
      },
      "source": [
        "##Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyKOYjAqUmIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bm25 = BM25Okapi(tokenized_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoFVo3CASVd7",
        "colab_type": "code",
        "outputId": "9e366340-4a21-4639-b8ae-c51fd498757a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "topic_ids = queries_df.topic_id\n",
        "topic_bm25at = []\n",
        "for topic_id in topic_ids:\n",
        "    print(topic_id)\n",
        "\n",
        "    # Compute ideal judgement:\n",
        "    ideal = judgements.loc[(judgements.topic_id == topic_id)]\n",
        "    ideal  = ideal.sort_values(by=['judgment'],ascending=False)\n",
        "    \n",
        "    query = queries_df.loc[queries_df['topic_id'] == topic_id][\"title\"].iloc[0]\n",
        "    query_in_out = queries_df.loc[queries_df['topic_id'] == topic_id][\"in_out_q\"].iloc[0]\n",
        "    query_in_in = queries_df.loc[queries_df['topic_id'] == topic_id][\"in_in_q\"].iloc[0]\n",
        "\n",
        "    queries = [query, query_in_out, query_in_in]\n",
        "    \n",
        "    topic_bm25 = []\n",
        "    for query in queries:\n",
        "        tokenized_query = prepros_query(query)\n",
        "        bm25_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "        bm25at = []\n",
        "        for at in [1,3,10,1000]:\n",
        "            top_doc = np.asarray(heapq.nlargest(at, range(len(bm25_scores)), bm25_scores.take))\n",
        "            top_doc_id = list(tokenized_df.iloc[top_doc][1])\n",
        "            \n",
        "            nDC = gain(top_doc_id, topic_id)\n",
        "            bm25at.append(nDC)\n",
        "        topic_bm25.append(bm25at)\n",
        "    topic_bm25at.append(topic_bm25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "321\n",
            "336\n",
            "341\n",
            "347\n",
            "350\n",
            "362\n",
            "363\n",
            "367\n",
            "375\n",
            "378\n",
            "393\n",
            "397\n",
            "400\n",
            "408\n",
            "414\n",
            "422\n",
            "426\n",
            "427\n",
            "433\n",
            "439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIBX9M1zOkj",
        "colab_type": "code",
        "outputId": "822a28d7-6e2d-4b43-b05c-708697b801e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "topic_bm25at[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0, 0.0, 0.11269280912425929, 0.2316099304108914],\n",
              " [0.0, 0.0, 0.04910158613120102, 0.23480543843234036],\n",
              " [0.0, 0.0, 0.04910158613120102, 0.10772106671898116]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siqcBa9NlXda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save file\n",
        "df_output = pd.DataFrame(topic_bm25at, columns = ['baseline', 'in_out_q',\"in_in_q\"]) \n",
        "output_file =  \"drive/My Drive/processed/output.csv\"\n",
        "df_output.to_csv(output_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NawUrMmSox5_",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate on Stemmed Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gfLyq0Nq8b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rQJwXkdXjs4",
        "colab_type": "text"
      },
      "source": [
        "##Load Stemmed Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovYvQck4o3gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pathfile = \"/content/drive/My Drive/processed/stemmed_data.parquet\"\n",
        "stemmed_df = pd.read_parquet(pathfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHETNOM6X_kf",
        "colab_type": "text"
      },
      "source": [
        "Convert to list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwBPGOe8pNTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmed_c = []\n",
        "for x in range(len(stemmed_df)):\n",
        "    stemmed_c.append(stemmed_df.iloc[x][2].tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSoKyxDpYLAu",
        "colab_type": "text"
      },
      "source": [
        "##Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A5a_Eac3pVBd",
        "colab": {}
      },
      "source": [
        "bm25 = BM25Okapi(stemmed_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4b9236a1-a5e3-4c7a-f947-af5f33128bfc",
        "id": "GywTCQ2-pXaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "topic_ids = queries_df.topic_id\n",
        "topic_bm25at = []\n",
        "for topic_id in topic_ids:\n",
        "    print(topic_id)\n",
        "\n",
        "    # Compute ideal judgement:\n",
        "    ideal = judgements.loc[(judgements.topic_id == topic_id)]\n",
        "    ideal  = ideal.sort_values(by=['judgment'],ascending=False)\n",
        "    \n",
        "    query = queries_df.loc[queries_df['topic_id'] == topic_id][\"title\"].iloc[0]\n",
        "    query_in_out = queries_df.loc[queries_df['topic_id'] == topic_id][\"in_out_q\"].iloc[0]\n",
        "    query_in_in = queries_df.loc[queries_df['topic_id'] == topic_id][\"in_in_q\"].iloc[0]\n",
        "\n",
        "    queries = [query, query_in_out, query_in_in]\n",
        "    \n",
        "    topic_bm25 = []\n",
        "    for query in queries:\n",
        "        tokenized_query = prepros_query(query)\n",
        "        stemmed_query = [stemmer.stem(w) for w in tokenized_query]\n",
        "        print(stemmed_query)\n",
        "        bm25_scores = bm25.get_scores(stemmed_query)\n",
        "\n",
        "        bm25at = []\n",
        "        for at in [1,3,10,1000]:\n",
        "            top_doc = np.asarray(heapq.nlargest(at, range(len(bm25_scores)), bm25_scores.take))\n",
        "            top_doc_id = list(tokenized_df.iloc[top_doc][1])\n",
        "            \n",
        "            nDC = gain(top_doc_id, topic_id)\n",
        "            bm25at.append(nDC)\n",
        "        topic_bm25.append(bm25at)\n",
        "    topic_bm25at.append(topic_bm25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "321\n",
            "['women', 'parliament']\n",
            "['women', 'parliament', 'european', 'elect']\n",
            "['women', 'parliament', 'men', 'femal']\n",
            "336\n",
            "['black', 'bear', 'attack']\n",
            "['black', 'bear', 'attack', 'panther', 'grizzli', 'moos']\n",
            "['black', 'bear', 'attack', 'white', 'red', 'big']\n",
            "341\n",
            "['airport', 'secur']\n",
            "['airport', 'secur', 'lax', 'dfw']\n",
            "['airport', 'secur', 'bwi', 'transport']\n",
            "347\n",
            "['wildlif', 'extinct']\n",
            "['wildlif', 'extinct', 'geograph', 'speci']\n",
            "['wildlif', 'extinct', 'wetland', 'conserv']\n",
            "350\n",
            "['health', 'comput', 'termin']\n",
            "['health', 'comput', 'termin', 'integr', 'connect', 'allianc']\n",
            "['health', 'comput', 'termin', 'system', 'technolog', 'peripher']\n",
            "362\n",
            "['human', 'smuggl']\n",
            "['human', 'smuggl', 'traffick', 'convict']\n",
            "['human', 'smuggl', 'traffick', 'illicit']\n",
            "363\n",
            "['transport', 'tunnel', 'disast']\n",
            "['transport', 'tunnel', 'disast', 'railroad', 'rail', 'termin']\n",
            "['transport', 'tunnel', 'disast', 'pollut', 'rout', 'highway']\n",
            "367\n",
            "['piraci']\n",
            "['piraci', 'anti']\n",
            "['piraci', 'cybercrim']\n",
            "375\n",
            "['hydrogen', 'energi']\n",
            "['hydrogen', 'energi', 'renew', 'effici']\n",
            "['hydrogen', 'energi', 'nitrogen', 'electr']\n",
            "378\n",
            "['euro', 'opposit']\n",
            "['euro', 'opposit', 'european', 'usdollar']\n",
            "['euro', 'opposit', 'european', 'imperialist']\n",
            "393\n",
            "['merci', 'kill']\n",
            "['merci', 'kill', 'victim', 'admit']\n",
            "['merci', 'kill', 'rape', 'curs']\n",
            "397\n",
            "['automobil', 'recal']\n",
            "['automobil', 'recal', 'vehicl', 'manufactur']\n",
            "['automobil', 'recal', 'vehicl', 'car']\n",
            "400\n",
            "['amazon', 'rain', 'forest']\n",
            "['amazon', 'rain', 'forest', 'trilog', 'mist', 'rainforest']\n",
            "['amazon', 'rain', 'forest', 'fire', 'snow', 'wind']\n",
            "408\n",
            "['tropic', 'storm']\n",
            "['tropic', 'storm', 'cristobol', 'thunderstorm']\n",
            "['tropic', 'storm', 'cristobol', 'hurrican']\n",
            "414\n",
            "['cuba', 'sugar', 'export']\n",
            "['cuba', 'sugar', 'export', 'import', 'chile', 'produc']\n",
            "['cuba', 'sugar', 'export', 'tariff', 'countri', 'import']\n",
            "422\n",
            "['art', 'stolen', 'forg']\n",
            "['art', 'stolen', 'forg', 'sword', 'sculptur', 'plate']\n",
            "['art', 'stolen', 'forg', 'collector', 'engrav', 'artwork']\n",
            "426\n",
            "['law', 'enforc', 'dog']\n",
            "['law', 'enforc', 'dog', 'regard', 'prohibit', 'violat']\n",
            "['law', 'enforc', 'dog', 'legal', 'violat', 'offic']\n",
            "427\n",
            "['uv', 'damag', 'eye']\n",
            "['uv', 'damag', 'eye', 'len', 'protect', 'glare']\n",
            "['uv', 'damag', 'eye', 'sunlight', 'skin', 'glare']\n",
            "433\n",
            "['greek', 'philosophi', 'stoicism']\n",
            "['greek', 'philosophi', 'stoicism', 'religion', 'christian', 'philosoph']\n",
            "['greek', 'philosophi', 'stoicism', 'existenti', 'aristotl', 'polyth']\n",
            "439\n",
            "['invent', 'scientif', 'discoveri']\n",
            "['invent', 'scientif', 'discoveri', 'scientist', 'experi', 'innov']\n",
            "['invent', 'scientif', 'discoveri', 'scientist', 'experi', 'advanc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PQkHjv5ssAex",
        "colab": {}
      },
      "source": [
        "# save file\n",
        "df_output = pd.DataFrame(topic_bm25at, columns = ['baseline', 'in_out_q',\"in_in_q\"]) \n",
        "output_file =  \"drive/My Drive/processed/output_stemmed.csv\"\n",
        "df_output.to_csv(output_file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}